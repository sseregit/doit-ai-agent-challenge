import streamlit as st
from dotenv import load_dotenv
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI

load_dotenv()

llm = ChatOpenAI(model="gpt-4o-mini")


def get_ai_response(messages):
  response = llm.stream(messages)

  for chunk in response:
    yield chunk  # ìƒì„±ëœ ì‘ë‹µì˜ ë‚´ìš©ì„ yieldë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.


st.title("ğŸ’¬ GPT-4o Langchain Chat")

if "messages" not in st.session_state:
  st.session_state["messages"] = [
    SystemMessage("ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤."),
    AIMessage("How can I help you?")
  ]

for msg in st.session_state.messages:
  if msg:
    if isinstance(msg, SystemMessage):
      st.chat_message("system").write(msg.content)
    elif isinstance(msg, AIMessage):
      st.chat_message("assistant").write(msg.content)
    elif isinstance(msg, HumanMessage):
      st.chat_message("user").write(msg.content)

if prompt := st.chat_input():
  st.chat_message("user").write(prompt)
  st.session_state.messages.append(HumanMessage(prompt))

  response = get_ai_response(st.session_state["messages"])

  result = st.chat_message("assistant").write_stream(response)
  st.session_state["messages"].append(AIMessage(result))
